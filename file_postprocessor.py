import zmq
import time
from Models.embedding import Embedding
from Models.llm import LLM, LLM_small
from db import create_data, delete_data

#í´ë¼ì´ì–¸íŠ¸ëŠ” ìš”ì•½í•˜ëŠ” ì• ëŠ” ì „ë¶€ë‹¤ sllmìœ¼ë¡œ ìˆ˜ì • í•„ìš”
class FilePostprocessor:
    def __init__(self, pull_port=5558):
        self.context = zmq.Context()
        self.pull_socket = self.context.socket(zmq.PULL)
        self.pull_socket.connect(f"tcp://localhost:{pull_port}")
        self.running = False

    def handle_create(self, message):
        """íŒŒì¼ ìƒì„± ì²˜ë¦¬"""
        file_path = message.get('file_path')
        content = message.get('content')
        print(f"ğŸ”¨ [CREATE] íŒŒì¼ ìƒì„± ì²˜ë¦¬ ì‹œì‘: {file_path}")

        if content:
            print(f"   ğŸ“ í…ìŠ¤íŠ¸ ì²­í‚¹ ì‹œì‘...")
            chunks, offsets = self._chunk_content(content, file_path)
            print(f"   âœ… ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
            
            print(f"   ğŸ” ì„ë² ë”© ìƒì„± ì‹œì‘...")
            embeddings = self._process_content(chunks, file_path, offsets)
            print(f"   âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(embeddings)}ê°œ ë²¡í„°")
            
            print(f"   ğŸ’¾ ChromaDB ì—…ë¡œë“œ ì‹œì‘...")
            self._upload_embeddings(embeddings, file_path)
            print(f"   âœ… ChromaDB ì—…ë¡œë“œ ì™„ë£Œ")
            
            print(f"   ğŸ“Š ìš”ì•½ ìƒì„± ì‹œì‘...")
            self._summarize(chunks, file_path)
            print(f"   âœ… ìš”ì•½ ìƒì„± ì™„ë£Œ")
        else:
            print(f"   âš ï¸ íŒŒì¼ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")
            
        print(f"ğŸ‰ [CREATE] íŒŒì¼ ìƒì„± ì²˜ë¦¬ ì™„ë£Œ: {file_path}")
        print("   " + "=" * 50)

    def handle_update(self, message):
        """íŒŒì¼ ìˆ˜ì • ì²˜ë¦¬"""
        file_path = message.get('file_path')
        content = message.get('content')
        diff_content = message.get('diff_content')
        
        print(f"ğŸ”„ [UPDATE] íŒŒì¼ ìˆ˜ì • ì²˜ë¦¬ ì‹œì‘: {file_path}")
        
        print(f"   ğŸ—‘ï¸ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì¤‘...")
        delete_data(file_path)
        print(f"   âœ… ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ")
        
        if diff_content: 
            print(f"   ğŸ“Š ë³€ê²½ì‚¬í•­ ìš”ì•½ ìƒì„± ì¤‘...")
            summary = LLM_small(f"ë‹¤ìŒ ë³€ê²½ì‚¬í•­ì„ 1~2ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”: {diff_content}")
            file_name = file_path.split('\\')[-1]
            print(f"   ğŸ“‹ {file_name} íŒŒì¼ì´ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
            print(f"   ğŸ“ ê²½ë¡œ: {file_path}")
            print(f"   ğŸ“ ë³€ê²½ì‚¬í•­: {summary}")
        
        if content:
            print(f"   ğŸ“ í…ìŠ¤íŠ¸ ì²­í‚¹ ì‹œì‘...")
            chunks, offsets = self._chunk_content(content, file_path)
            print(f"   âœ… ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
            
            print(f"   ğŸ” ì„ë² ë”© ìƒì„± ì‹œì‘...")
            embeddings = self._process_content(chunks, file_path, offsets)
            print(f"   âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(embeddings)}ê°œ ë²¡í„°")
            
            print(f"   ğŸ’¾ ChromaDB ì—…ë¡œë“œ ì‹œì‘...")
            self._upload_embeddings(embeddings, file_path)
            print(f"   âœ… ChromaDB ì—…ë¡œë“œ ì™„ë£Œ")
        else:
            print(f"   âš ï¸ íŒŒì¼ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")
            
        print(f"ğŸ‰ [UPDATE] íŒŒì¼ ìˆ˜ì • ì²˜ë¦¬ ì™„ë£Œ: {file_path}")
        print("   " + "=" * 50)

    def handle_delete(self, message):
        """íŒŒì¼ ì‚­ì œ ì²˜ë¦¬"""
        file_path = message.get('file_path')
        print(f"ğŸ—‘ï¸ [DELETE] íŒŒì¼ ì‚­ì œ ì²˜ë¦¬ ì‹œì‘: {file_path}")
        
        print(f"   ğŸ’¾ ChromaDBì—ì„œ ë°ì´í„° ì‚­ì œ ì¤‘...")
        delete_data(file_path)
        print(f"   âœ… ChromaDB ë°ì´í„° ì‚­ì œ ì™„ë£Œ")
        
        file_name = file_path.split('\\')[-1] if '\\' in file_path else file_path.split('/')[-1]
        print(f"   ğŸ“‹ {file_name} íŒŒì¼ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"   ğŸ“ ê²½ë¡œ: {file_path}")
        
        print(f"ğŸ‰ [DELETE] íŒŒì¼ ì‚­ì œ ì²˜ë¦¬ ì™„ë£Œ: {file_path}")
        print("   " + "=" * 50)

    def _chunk_content(self, content, file_path):
        """í…ìŠ¤íŠ¸ ì²­í‚¹"""
        if not content or not content.strip():
            return [], []
        
        # 1. contentë¥¼ overlapê³¼ í•¨ê»˜ ì‘ì€ chunkë“¤ë¡œ ë‚˜ëˆ„ê¸°
        pre_chunks = self._split_with_overlap(content, 1000, 200)
        
        # 2. ê° pre_chunkì—ì„œ LLMìœ¼ë¡œ ì˜ë¯¸ì  ëì ì˜ ë§ˆì§€ë§‰ ë¬¸ì¥ë“¤ ì¶”ì¶œ
        all_end_sentences = []
        for pre_chunk in pre_chunks:
            end_sentences = self._extract_end_sentences(pre_chunk)
            all_end_sentences.extend(end_sentences)
        
        # 3. ë§ˆì§€ë§‰ ë¬¸ì¥ë“¤ì˜ ìœ„ì¹˜ë¥¼ ì°¾ì•„ì„œ contentë¥¼ ìµœì¢… ë¶„í• 
        final_chunks = self._split_by_end_sentences(content, all_end_sentences)
        
        chunks = [chunk["text"] for chunk in final_chunks]
        offsets = [{
            "chunk_index": chunk["chunk_index"],
            "char_start": chunk["char_start"],
            "char_end": chunk["char_end"],
            "word_start": chunk["word_start"],
            "word_end": chunk["word_end"]
        } for chunk in final_chunks]
        
        return chunks, offsets

    def _split_with_overlap(self, content, chunk_size, overlap):
        """contentë¥¼ ì§€ì •ëœ í¬ê¸°ë¡œ overlapê³¼ í•¨ê»˜ ë¶„í• """
        chunks = []
        start = 0
        
        while start < len(content):
            end = min(start + chunk_size, len(content))
            chunk = content[start:end]
            chunks.append(chunk)
            
            if end >= len(content):
                break
                
            start = end - overlap
        
        return chunks

    def _extract_end_sentences(self, text):
        """LLMì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì—ì„œ ì˜ë¯¸ì ìœ¼ë¡œ ëë‚˜ëŠ” ì§€ì ì˜ ë§ˆì§€ë§‰ ë¬¸ì¥ë“¤ì„ ì¶”ì¶œ"""
        prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ì˜ë¯¸ì ìœ¼ë¡œ ì™„ê²°ë˜ëŠ” ì§€ì ë“¤ì˜ ë§ˆì§€ë§‰ ë¬¸ì¥ì„ ì°¾ì•„ì£¼ì„¸ìš”.
ê° ë§ˆì§€ë§‰ ë¬¸ì¥ì„ í•œ ì¤„ì— í•˜ë‚˜ì”© ì¶œë ¥í•´ì£¼ì„¸ìš”.

í…ìŠ¤íŠ¸:
{text}

ë§ˆì§€ë§‰ ë¬¸ì¥ë“¤:"""
        
        try:
            response = LLM_small(prompt)
            sentences = [line.strip() for line in response.split('\n') if line.strip()]
            return sentences
        except Exception as e:
            print(f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return []

    def _split_by_end_sentences(self, content, end_sentences):
        """ë§ˆì§€ë§‰ ë¬¸ì¥ë“¤ì˜ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ contentë¥¼ ìµœì¢… ë¶„í• """
        if not end_sentences:
            # LLM ì‹¤íŒ¨ì‹œ ë‹¨ìˆœ ë¶„í• 
            return [{"chunk_index": 0, "text": content, "char_start": 0, "char_end": len(content), 
                    "word_start": 0, "word_end": len(content.split()) - 1}]
        
        chunks = []
        last_end = 0
        chunk_index = 0
        
        for end_sentence in end_sentences:
            end_pos = content.find(end_sentence, last_end)
            if end_pos == -1:
                continue
                
            actual_end = end_pos + len(end_sentence)
            chunk_text = content[last_end:actual_end].strip()
            if not chunk_text:
                continue
            
            chunk = {
                "chunk_index": chunk_index,
                "text": chunk_text,
                "char_start": last_end,
                "char_end": actual_end,
                "word_start": len(content[:last_end].split()),
                "word_end": len(content[:actual_end].split()) - 1
            }
            
            chunks.append(chunk)
            last_end = actual_end
            chunk_index += 1
        
        # ë§ˆì§€ë§‰ ë‚¨ì€ ë¶€ë¶„ ì¶”ê°€
        if last_end < len(content):
            remaining_text = content[last_end:].strip()
            if remaining_text:
                chunk = {
                    "chunk_index": chunk_index,
                    "text": remaining_text,
                    "char_start": last_end,
                    "char_end": len(content),
                    "word_start": len(content[:last_end].split()),
                    "word_end": len(content.split()) - 1
                }
                chunks.append(chunk)
        
        return chunks

    def _process_content(self, chunks, file_path, offsets):
        """ì„ë² ë”© ë°°ì¹˜ ìƒì„±"""
        # ëª¨ë“  chunksë¥¼ í•œ ë²ˆì— ë°°ì¹˜ ì²˜ë¦¬
        embedding_vectors = Embedding(chunks)
        
        # ì„ë² ë”©ê³¼ ì˜¤í”„ì…‹ ì •ë³´ë¥¼ ë§¤í•‘í•˜ì—¬ ë°˜í™˜
        embeddings = []
        for i, (embedding_vector, chunk_text) in enumerate(zip(embedding_vectors, chunks)):
            embeddings.append({
                'embedding': embedding_vector,
                'text': chunk_text,
                'offset': offsets[i]
            })
        
        return embeddings

    def _summarize(self, chunks, file_path):
        """ë°°ì¹˜ ì²­í¬ ìš”ì•½ í›„ ìµœì¢… ìš”ì•½"""
        print(f"       ğŸ“Š ê°œë³„ ì²­í¬ ìš”ì•½ ìƒì„± ì¤‘... ({len(chunks)}ê°œ ì²­í¬)")
        
        # ëª¨ë“  chunksë¥¼ í•œ ë²ˆì— ë°°ì¹˜ ì²˜ë¦¬ë¡œ ìš”ì•½
        prompts = [f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ 1~2ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”: {chunk}" for chunk in chunks]
        chunk_summaries = [LLM_small(prompt) for prompt in prompts]
        
        print(f"       âœ… ê°œë³„ ì²­í¬ ìš”ì•½ ì™„ë£Œ")
        print(f"       ğŸ“ ìµœì¢… ìš”ì•½ ìƒì„± ì¤‘...")
        
        # ìš”ì•½ë“¤ì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ìš”ì•½
        combined_summaries = "\n".join(chunk_summaries)
        final_summary = LLM_small(f"ë‹¤ìŒ ìš”ì•½ë“¤ì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ìš”ì•½ì„ 2~3ë¬¸ì¥ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”: {combined_summaries}")
        
        file_name = file_path.split('\\')[-1]
        print(f"       ğŸ“‹ {file_name} íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"       ğŸ“ ê²½ë¡œ: {file_path}")
        print(f"       ğŸ“ ë‚´ìš©: {final_summary}")
        print(f"       âœ… ìµœì¢… ìš”ì•½ ì™„ë£Œ")

    def _upload_embeddings(self, embeddings, file_path):
        """ì„ë² ë”©ì„ ChromaDBì— ì—…ë¡œë“œ"""
        print(f"       ğŸ’¾ ChromaDB ì—…ë¡œë“œ ì§„í–‰ ì¤‘... ({len(embeddings)}ê°œ ì„ë² ë”©)")
        
        success_count = 0
        for i, embedding_data in enumerate(embeddings):
            try:
                create_data(
                    file_path=file_path,
                    start_idx=embedding_data['offset']['char_start'],
                    end_idx=embedding_data['offset']['char_end'],
                    embedding=embedding_data['embedding']
                )
                success_count += 1
                
                # ì§„í–‰ë¥  í‘œì‹œ (10% ë‹¨ìœ„)
                progress = (i + 1) / len(embeddings) * 100
                if (i + 1) % max(1, len(embeddings) // 10) == 0 or i == len(embeddings) - 1:
                    print(f"       ğŸ“ˆ ì—…ë¡œë“œ ì§„í–‰ë¥ : {progress:.0f}% ({i + 1}/{len(embeddings)})")
                    
            except Exception as e:
                print(f"       âŒ ì„ë² ë”© ì—…ë¡œë“œ ì‹¤íŒ¨ (ì²­í¬ {i}): {e}")
        
        print(f"       âœ… ChromaDB ì—…ë¡œë“œ ì™„ë£Œ: {success_count}/{len(embeddings)} ì„±ê³µ")

    def process_message(self, message):
        """ë©”ì‹œì§€ ì²˜ë¦¬"""
        event_type = message.get('event_type')
        file_path = message.get('file_path')
        user_id = message.get('user_id')
        timestamp = message.get('timestamp')
        processed_timestamp = message.get('processed_timestamp')
        status = message.get('status')
        
        # ë©”ì‹œì§€ ìˆ˜ì‹  ë¡œê·¸ ì¶œë ¥
        print(f"ğŸ“¥ [RECEIVE <- file_preprocessor] íŒŒì¼ ì²˜ë¦¬ ë©”ì‹œì§€ ìˆ˜ì‹ ")
        print(f"   ğŸ“„ íŒŒì¼: {file_path}")
        print(f"   ğŸ“‹ ì´ë²¤íŠ¸: {event_type}")
        print(f"   ğŸ‘¤ ì‚¬ìš©ì: {user_id}")
        print(f"   âœ… ì „ì²˜ë¦¬ ìƒíƒœ: {status}")
        
        if timestamp:
            print(f"   ğŸ“… ì›ë³¸ íƒ€ì„ìŠ¤íƒ¬í”„: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(timestamp))}")
        if processed_timestamp:
            print(f"   ğŸ“… ì²˜ë¦¬ íƒ€ì„ìŠ¤íƒ¬í”„: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(processed_timestamp))}")
        
        content = message.get('content')
        if content:
            content_length = message.get('content_length', len(content))
            print(f"   ğŸ“ ë‚´ìš© ê¸¸ì´: {content_length:,} ë¬¸ì")
        
        if event_type == 'update':
            diff_type = message.get('diff_type')
            diff_content = message.get('diff_content')
            if diff_type:
                print(f"   ğŸ“Š Diff íƒ€ì…: {diff_type}")
                if diff_content:
                    print(f"   ğŸ“Š Diff í¬ê¸°: {len(diff_content)} chars")
        
        print("   " + "-" * 50)
        
        # ì‹¤ì œ ì²˜ë¦¬ ë¡œì§ ì‹¤í–‰
        if event_type == 'create':
            self.handle_create(message)
        elif event_type == 'update':
            self.handle_update(message)
        elif event_type == 'delete':
            self.handle_delete(message)
        else:
            print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ì´ë²¤íŠ¸ íƒ€ì…: {event_type}")

    def start(self):
        """ì„œë¹„ìŠ¤ ì‹œì‘"""
        self.running = True
        print("File Postprocessor ì‹œì‘...")

        try:
            while self.running:
                if self.pull_socket.poll(timeout=1000):
                    message = self.pull_socket.recv_json()
                    self.process_message(message)

        except KeyboardInterrupt:
            print("ì¢…ë£Œ ì¤‘...")
        finally:
            self.pull_socket.close()
            self.context.term()

if __name__ == "__main__":
    postprocessor = FilePostprocessor()
    postprocessor.start()