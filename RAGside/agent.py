"""
RAG Agent - ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰ê³¼ AI ì¶”ë¡ ì„ ë°˜ë³µí•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì—ì´ì „íŠ¸
"""

import json
from typing import Dict, Any, List, Optional
from Models.llm import structured_LLM
from retriever import FileRetriever


class RAGAgent:
    """RAG ì‹œìŠ¤í…œì„ ì´ìš©í•œ ì—ì´ì „íŠ¸"""
    
    def __init__(self, mode: str = "deep", user_id: str = None):
        """
        Args:
            mode: ê²€ìƒ‰ ëª¨ë“œ ('normal': 1íšŒ, 'deep': 3íšŒ, 'deeper': 5íšŒ)
            user_id: ì‚¬ìš©ì ID (ê¶Œí•œ í™•ì¸ìš©, í•„ìˆ˜)
        """
        if not user_id:
            raise ValueError("ì‚¬ìš©ì IDê°€ í•„ìš”í•©ë‹ˆë‹¤. ì ‘ê·¼ì´ ê±°ë¶€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        self.mode = mode
        self.user_id = user_id
        self.max_iterations = self._get_max_iterations()
        self.retriever = FileRetriever(user_id=user_id)
        
        # structured LLMì„ ìœ„í•œ JSON ìŠ¤í‚¤ë§ˆ
        self.output_schema = {
            "type": "object",
            "properties": {
                "llm_output": {
                    "type": "string",
                    "description": "AIì˜ ì‘ë‹µì´ë‚˜ ìƒê°"
                },
                "continue_iterate": {
                    "type": "boolean", 
                    "description": "ë” ê²€ìƒ‰ì´ í•„ìš”í•œì§€ ì—¬ë¶€"
                },
                "search_query": {
                    "type": "string",
                    "description": "ë‹¤ìŒì— ê²€ìƒ‰í•  ì¿¼ë¦¬ (continue_iterateê°€ trueì¼ ë•Œë§Œ ì‚¬ìš©)"
                }
            },
            "required": ["llm_output", "continue_iterate", "search_query"]
        }
    
    def _get_max_iterations(self) -> int:
        """ëª¨ë“œì— ë”°ë¥¸ ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ë°˜í™˜"""
        modes = {"normal": 1, "deep": 3, "deeper": 5}
        return modes.get(self.mode, 3)  # ì˜ëª»ëœ ê°’ì´ë©´ ê¸°ë³¸ê°’ 3
    
    def process(self, user_input: str, conversation_history: Optional[List[Dict[str, str]]] = None) -> str:
        """
        ì‚¬ìš©ì ì…ë ¥ì„ ì²˜ë¦¬í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ë°˜í™˜
        
        Args:
            user_input: ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ë‚˜ ìš”ì²­
            conversation_history: ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ (optional)
            
        Returns:
            ìµœì¢… AI ì‘ë‹µ
        """
        if conversation_history is None:
            conversation_history = []
            
        print(f"ğŸ¤– ì‚¬ìš©ì ì§ˆë¬¸: {user_input}")
        
        # ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ëˆ„ì 
        accumulated_context = ""
        last_search_results = []  # ë§ˆì§€ë§‰ ê²€ìƒ‰ ê²°ê³¼ ì €ì¥
        iteration = 0
        
        # ì²« ë²ˆì§¸ ê²€ìƒ‰ ì¿¼ë¦¬ëŠ” ì‚¬ìš©ì ì…ë ¥ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        current_query = user_input
        
        while iteration < self.max_iterations:
            iteration += 1
            print(f"\nğŸ”„ ë°˜ë³µ {iteration}/{self.max_iterations}")
            
            # 1. í˜„ì¬ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰ ìˆ˜í–‰
            print(f"ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: {current_query}")
            search_results = self.retriever.search_chunks(current_query, top_n=3)
            last_search_results = search_results  # ë§ˆì§€ë§‰ ê²€ìƒ‰ ê²°ê³¼ ì €ì¥
            
            if search_results:
                new_context = "\n".join(search_results)
                accumulated_context += f"\n\n=== ê²€ìƒ‰ ê²°ê³¼ {iteration} ===\n{new_context}"
                print(f"âœ… {len(search_results)}ê°œ ë¬¸ì„œ ì¡°ê° ë°œê²¬")
            else:
                print("âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            
            # 2. LLMì—ê²Œ í˜„ì¬ ìƒí™©ì„ ì „ë‹¬í•˜ê³  ë‹¤ìŒ ì•¡ì…˜ ê²°ì •
            prompt = self._build_prompt(user_input, accumulated_context, iteration, conversation_history)
            
            try:
                response = structured_LLM(prompt, self.output_schema)
                result = json.loads(response)
                
                print(f"ğŸ§  AI ì‘ë‹µ: {result['llm_output']}")
                print(f"ğŸ”„ ê³„ì† ê²€ìƒ‰ í•„ìš”: {result['continue_iterate']}")
                
                # Normal ëª¨ë“œëŠ” ì²« ê²€ìƒ‰ í›„ ë¬´ì¡°ê±´ ì¢…ë£Œ
                if self.mode == "normal":
                    print("âœ… Normal ëª¨ë“œ - ê²€ìƒ‰ ì™„ë£Œ")
                    self._show_referenced_chunks(last_search_results)
                    return result['llm_output']
                
                # ê³„ì† ê²€ìƒ‰ì´ í•„ìš”í•˜ì§€ ì•Šë‹¤ë©´ ì¢…ë£Œ
                if not result['continue_iterate']:
                    print("âœ… ê²€ìƒ‰ ì™„ë£Œ")
                    self._show_referenced_chunks(last_search_results)
                    return result['llm_output']
                
                # ë‹¤ìŒ ê²€ìƒ‰ ì¿¼ë¦¬ ì„¤ì •
                current_query = result['search_query']
                print(f"â¡ï¸ ë‹¤ìŒ ê²€ìƒ‰ ì¿¼ë¦¬: {current_query}")
                
            except Exception as e:
                print(f"âŒ LLM ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                break
        
        # ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ë„ë‹¬ ì‹œ ë§ˆì§€ë§‰ ì‹œë„
        print(f"\nâ° ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ë„ë‹¬. ìµœì¢… ë‹µë³€ ìƒì„±...")
        final_prompt = f"""
ì‚¬ìš©ì ì§ˆë¬¸: {user_input}

ìˆ˜ì§‘ëœ ì •ë³´:
{accumulated_context}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ìµœì¢… ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
"""
        
        try:
            final_response = structured_LLM(final_prompt, self.output_schema)
            final_result = json.loads(final_response)
            self._show_referenced_chunks(last_search_results)
            return final_result['llm_output']
        except Exception as e:
            print(f"âŒ ìµœì¢… ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def _build_prompt(self, user_input: str, context: str, iteration: int, conversation_history: List[Dict[str, str]]) -> str:
        """LLMìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        # íˆìŠ¤í† ë¦¬ í…ìŠ¤íŠ¸ ìƒì„± (ìµœê·¼ 5ê°œë§Œ ì‚¬ìš©)
        history_text = ""
        if conversation_history:
            recent_history = conversation_history[-5:]  # ìµœê·¼ 5ê°œë§Œ
            history_text = "\n".join([f"{msg['role']}: {msg['content']}" for msg in recent_history])
        
        return f"""
ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³  ë¶„ì„í•˜ëŠ” AI ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.

ëŒ€í™” íˆìŠ¤í† ë¦¬:
{history_text}

ì‚¬ìš©ì ì§ˆë¬¸: {user_input}

í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ëœ ì •ë³´:
{context}

í˜„ì¬ ë°˜ë³µ: {iteration}/{self.max_iterations}

ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”:
1. ì¶©ë¶„í•œ ì •ë³´ê°€ ìˆë‹¤ë©´ -> continue_iterate: false, ìµœì¢… ë‹µë³€ ì œê³µ
2. ë” ê²€ìƒ‰ì´ í•„ìš”í•˜ë‹¤ë©´ -> continue_iterate: true, ìƒˆë¡œìš´ ê²€ìƒ‰ ì¿¼ë¦¬ ì œì•ˆ

ì‘ë‹µ í˜•ì‹:
- llm_output: í˜„ì¬ ìƒí™©ì— ëŒ€í•œ ë‹¹ì‹ ì˜ ë¶„ì„ì´ë‚˜ ë‹µë³€
- continue_iterate: ë” ê²€ìƒ‰í• ì§€ ì—¬ë¶€ (true/false)  
- search_query: ë‹¤ìŒ ê²€ìƒ‰í•  ë‚´ìš© (continue_iterateê°€ trueì¼ ë•Œë§Œ)
"""
    
    def _show_referenced_chunks(self, chunks: list):
        """ë§ˆì§€ë§‰ì— ì°¸ê³ í•œ chunkë“¤ì„ í‘œì‹œ"""
        if not chunks:
            return
            
        print(f"\nğŸ“š ì°¸ê³ í•œ ë¬¸ì„œ ({len(chunks)}ê°œ):")
        print("=" * 50)
        for i, chunk in enumerate(chunks, 1):
            # chunk í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì•ë¶€ë¶„ë§Œ í‘œì‹œ
            preview = chunk[:200] + "..." if len(chunk) > 200 else chunk
            print(f"{i}. {preview}")
            print("-" * 30)
    
    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.retriever.close()


def main():
    """ì‚¬ìš© ì˜ˆì‹œ"""
    print("RAG Agent - ê²€ìƒ‰ ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”:")
    print("1. normal - ë¹ ë¥¸ ê²€ìƒ‰ (1íšŒ)")
    print("2. deep - ì¼ë°˜ ê²€ìƒ‰ (3íšŒ, ê¸°ë³¸ê°’)")
    print("3. deeper - ì‹¬í™” ê²€ìƒ‰ (5íšŒ)")
    
    mode_input = input("ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš” (1/2/3, ì—”í„°=ê¸°ë³¸ê°’): ").strip()
    mode_map = {"1": "normal", "2": "deep", "3": "deeper"}
    mode = mode_map.get(mode_input, "deep")
    
    # ì‚¬ìš©ì ID ì…ë ¥
    user_id = input("ì‚¬ìš©ì IDë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
    if not user_id:
        print("âŒ ì‚¬ìš©ì IDê°€ í•„ìš”í•©ë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    try:
        agent = RAGAgent(mode, user_id=user_id)
        print(f"ğŸš€ {mode} ëª¨ë“œë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
        print(f"ğŸ‘¤ ì‚¬ìš©ì: {user_id}")
    except ValueError as e:
        print(f"âŒ {e}")
        return
    
    try:
        # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
        user_question = input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ")
        
        # ì²˜ë¦¬ ë° ê²°ê³¼ ì¶œë ¥
        answer = agent.process(user_question)
        print(f"\nğŸ¯ ìµœì¢… ë‹µë³€:\n{answer}")
        
    except KeyboardInterrupt:
        print("\ní”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        agent.close()


if __name__ == "__main__":
    main()